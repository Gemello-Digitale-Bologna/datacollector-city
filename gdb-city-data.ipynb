{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6effd0c6-c127-4499-92aa-963101c4601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import digitalhub as dh\n",
    "import os\n",
    "PROJECT_NAME = \"city-data\"\n",
    "project = dh.get_or_create_project(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba9323-b2bd-4fff-a494-193efd5f3e37",
   "metadata": {},
   "source": [
    "## Open Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b7ab47-2d1d-449e-bbf1-43bea0861cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder ='src'\n",
    "if not os.path.exists(new_folder):\n",
    "    os.makedirs(new_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4244514-4c10-4834-9608-5de8b4de1a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/download-open-data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/download-open-data.py\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json as js\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import boto3\n",
    "import datetime\n",
    "\n",
    "base_url = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/{name}/exports/geojson\"\n",
    "\n",
    "def download_share_geo(project, bucket, name, artifact_name):\n",
    "    json = requests.get(base_url.format(name = name)).json()  \n",
    "    if not os.path.exists('./data'):\n",
    "        os.makedirs('./data')\n",
    "\n",
    "    path_geojson = './data/'+name+'.geojson' \n",
    "    with open(path_geojson, 'w') as out_file:\n",
    "            out_file.write(js.dumps(json, indent=4))\n",
    "    gdf = gpd.read_file(path_geojson)\n",
    "    # share_files(project, bucket, path_geojson, artifact_name)\n",
    "    \n",
    "    path_parquet = './data/'+name+'.parquet'\n",
    "    gdf.to_parquet(path_parquet)\n",
    "    share_files(project, bucket, path_parquet, artifact_name)  \n",
    "\n",
    "def download_buildings(project, bucket):\n",
    "    data = download_share_geo(project, bucket, \"rifter_edif_pl\", \"rifter_buildings_censiment\")\n",
    "\n",
    "def download_addresses(project, bucket):\n",
    "    data = download_share_geo(project, bucket, \"rifter_civici_pt\", \"rifter_addresses\")\n",
    "\n",
    "def download_walls(project, bucket):\n",
    "    data = download_share_geo(project, bucket, \"carta-tecnica-comunale-recinzioni\", \"ctm_walls\")\n",
    "\n",
    "def download_buidlings_volumes(project, bucket):\n",
    "    data = download_share_geo(project, bucket, \"c_a944ctc_edifici_pl\", \"ctm_buildings_volumes\")\n",
    "\n",
    "def download_buildings_symbols(project, bucket):\n",
    "    data = download_share_geo(project, bucket, \"carta-tecnica-comunale-simboli-edifici-volumetrici\", \"ctm_buildings_symbols\")\n",
    "\n",
    "def download_terrain(project, bucket):\n",
    "    data = download_share_geo(project, bucket, \"carta-tecnica-comunale-divisioni-del-terreno\", \"ctm_terrain\")\n",
    "    \n",
    "def download_areas(project, bucket):\n",
    "    data = download_share_geo(project, bucket, \"zone_urbanistiche\", \"proximity_areas\")\n",
    "\n",
    "def share_files(project, bucket: str = \"dataspace\", path: str = \"city\", artifactName: str = 'artifact'):\n",
    "    \"\"\"\n",
    "    Uploads specified data items to a shared S3 bucket and folder.\n",
    "    Requires the environment variables for S3 endpoint and credentials (S3_ENDPOINT_URL, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY).\n",
    "    args:\n",
    "        bucket: The name of the bucket\n",
    "        path: The path within the bucket\n",
    "    \"\"\"\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "                    endpoint_url=os.environ.get('S3_ENDPOINT_URL'),\n",
    "                    aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                    aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "    \n",
    "    path_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    path_latest = 'latest'\n",
    "    \n",
    "    fname = path.split(\"/\")[-1]\n",
    "    name = fname.split(\".\")[0]\n",
    "\n",
    "    print(bucket)\n",
    "    s3.upload_file(\"./data/\" +fname, bucket, '/' + artifactName + '/' + path_latest + '/' + fname, ExtraArgs={'ContentType': 'application/octet-stream'})\n",
    "    s3.upload_file(\"./data/\" +fname, bucket, '/' + artifactName + '/' + path_date + '/' + fname, ExtraArgs={'ContentType': 'application/octet-stream'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb9814c-1e96-4d64-a057-986f8424eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_buildings = project.new_function(name=\"download-buildings\",\n",
    "                                               kind=\"python\",\n",
    "                                               python_version=\"PYTHON3_10\",\n",
    "                                               source={\"source\": \"src/download-open-data.py\", \"handler\": \"download_buildings\"},\n",
    "                                               requirements= [\"geopandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c46404-5465-4a74-8d1d-a4c96acd6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_buildings = func_download_buildings.run(action=\"job\", parameters={'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5135621a-e01e-4c68-87fc-334dd3081ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_addresses = project.new_function(name=\"download-addresses\",\n",
    "                                               kind=\"python\",\n",
    "                                               python_version=\"PYTHON3_10\",\n",
    "                                               source={\"source\": \"src/download-open-data.py\", \"handler\": \"download_addresses\"},\n",
    "                                               requirements= [\"geopandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50cd449-9866-44c5-a492-6a019de5a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_addresses = func_download_addresses.run(action=\"job\", parameters={'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee217d7-c86d-4acb-97c9-672a2e10e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_walls = project.new_function(name=\"download-walls\",\n",
    "                                               kind=\"python\",\n",
    "                                               python_version=\"PYTHON3_10\",\n",
    "                                               source={\"source\": \"src/download-open-data.py\", \"handler\": \"download_walls\"},\n",
    "                                               requirements= [\"geopandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ecb3d0-0598-4e46-8d8a-5b89a74b097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_walls = func_download_walls.run(action=\"job\", parameters={'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9630c407-5f8f-4edd-bd78-2ea4db256bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_buidlings_volumes = project.new_function(name=\"download-buidlings-volumes\",\n",
    "                                               kind=\"python\",\n",
    "                                               python_version=\"PYTHON3_10\",\n",
    "                                               source={\"source\": \"src/download-open-data.py\", \"handler\": \"download_buidlings_volumes\"},\n",
    "                                               requirements= [\"geopandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9223a11-f72b-4a11-b6c2-96cb47dabfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_buidlings_volumes = func_download_buidlings_volumes.run(action=\"job\", parameters={'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f296319d-2434-4ed4-8bfb-16cd48dfce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_buildings_symbols = project.new_function(name=\"download-buildings-symbols\",\n",
    "                                               kind=\"python\",\n",
    "                                               python_version=\"PYTHON3_10\",\n",
    "                                               source={\"source\": \"src/download-open-data.py\", \"handler\": \"download_buildings_symbols\"},\n",
    "                                               requirements= [\"geopandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acbfb792-bc5f-4338-849e-8ea7dd87d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_buildings_symbols = func_download_buildings_symbols.run(action=\"job\", parameters={'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295ac699-db5e-428e-9bb5-a51cea0e4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_terrain = project.new_function(name=\"download-terrain\",\n",
    "                                               kind=\"python\",\n",
    "                                               python_version=\"PYTHON3_10\",\n",
    "                                               source={\"source\": \"src/download-open-data.py\", \"handler\": \"download_terrain\"},\n",
    "                                               requirements= [\"geopandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6f61d7b-c8fb-4e9c-8601-34deb1806908",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_terrain = func_download_terrain.run(action=\"job\", parameters={'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed32be2-1536-439b-bab0-711d473ba0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_areas = project.new_function(name=\"download-proximity-areas\",\n",
    "                                               kind=\"python\",\n",
    "                                               python_version=\"PYTHON3_10\",\n",
    "                                               source={\"source\": \"src/download-open-data.py\", \"handler\": \"download_areas\"},\n",
    "                                               requirements= [\"geopandas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f867ffc-145c-40dd-b6bf-34dc8981a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_areas = func_download_areas.run(action=\"job\", parameters={'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1e6a6-ee79-440f-b9da-8557835e71bf",
   "metadata": {},
   "source": [
    "## Google Drive Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46fea32-0dba-44c8-b873-59e12de2ec7a",
   "metadata": {},
   "source": [
    "Upload the token file as aritifact in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f33a5a9-667e-4e07-8900-bd1bf834ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_uri = project.get_artifact('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6ea39c4-a99f-4df6-9676-27574639a198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/download-data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/download-data.py\"\n",
    "\n",
    "import os\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive.readonly\"]\n",
    "\n",
    "\n",
    "def getGService(project, token_uri):\n",
    "    creds = None\n",
    "    token = token_uri.as_file()\n",
    "    try:\n",
    "        token_info = json.load(open(token))\n",
    "        creds = Credentials.from_authorized_user_info(token_info, SCOPES)\n",
    "        service = build(\"drive\", \"v3\", credentials=creds)\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "    return service\n",
    "\n",
    "def upload_file(s3, bucket: str, path: str, local_path: str, item_name: str):\n",
    "    \"\"\"\n",
    "    Uploads specified data items to a shared S3 bucket and folder.\n",
    "    Requires the environment variables for S3 endpoint and credentials (S3_ENDPOINT_URL, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY).\n",
    "    args:\n",
    "        bucket: The name of the bucket\n",
    "        path: The path within the bucket\n",
    "    \"\"\"\n",
    "\n",
    "    name = path + '/' + item_name\n",
    "    s3.upload_file(local_path, bucket, name, ExtraArgs={'ContentType': 'application/octet-stream'})\n",
    "\n",
    "def copy_rec(service, folder_id, s3, bucket: str, destination_path: str):\n",
    "    \"\"\"Downloads recursively all content from a specific folder on Google Drive.\"\"\"\n",
    "    files = service.files()\n",
    "    request = files.list(q=f\"'{folder_id}' in parents\", \n",
    "                         supportsAllDrives=True, includeItemsFromAllDrives=True, \n",
    "                         fields=\"nextPageToken, files(id, name, mimeType)\")\n",
    "    while request is not None:\n",
    "        results = request.execute()\n",
    "        \n",
    "        items = results.get(\"files\", [])\n",
    "        for item in items:\n",
    "            item_name = item[\"name\"]\n",
    "            item_id = item[\"id\"]\n",
    "            item_type = item[\"mimeType\"]\n",
    "\n",
    "            # If it's a folder, recursively download its content\n",
    "            if item_type == \"application/vnd.google-apps.folder\":\n",
    "                print(f\"Downloading folder {item_name}...\")\n",
    "                if item_name != 'back':\n",
    "                    copy_rec(service, item_id, s3, bucket, destination_path + '/' + item_name)\n",
    "            else:\n",
    "                try:\n",
    "                    head = s3.head_object(Bucket=bucket, Key=destination_path + '/' + item_name)\n",
    "                except:\n",
    "                    print(f\"Downloading file {item_name}...\")\n",
    "                    # Download the file into the specified local folder\n",
    "                    r = service.files().get_media(fileId=item_id)\n",
    "                    local_path = 'myfile'\n",
    "                    with open(local_path, \"wb\") as fh:\n",
    "                        downloader = MediaIoBaseDownload(fh, r)\n",
    "                        done = False\n",
    "                        while not done:\n",
    "                            status, done = downloader.next_chunk()\n",
    "                    upload_file(s3, bucket, destination_path, local_path, item_name)\n",
    "                    # return\n",
    "        request = files.list_next(request, results)\n",
    "\n",
    "def copy_files(project, query: str, s3, bucket: str, destination_path: str, token_uri):\n",
    "    service = getGService(project, token_uri)\n",
    "    results = (service.files()\n",
    "               .list(q=query, pageSize=1, fields=\"files(id, name, mimeType)\", supportsAllDrives=True, includeItemsFromAllDrives=True)\n",
    "               .execute())\n",
    "    print(results.get(\"files\"))\n",
    "    root_items = results.get(\"files\", [])\n",
    "    for item in root_items:\n",
    "        copy_rec(service, item[\"id\"], s3, bucket, destination_path)\n",
    "\n",
    "def doc_files(s3, bucket: str, prefix: str, name: str):\n",
    "    array = []\n",
    "    res = s3.list_objects(Bucket=bucket, Prefix=prefix, MaxKeys=1000)\n",
    "    array = array + res['Contents']\n",
    "    while 'NextMarker' in res:\n",
    "        res = s3.list_objects(Bucket=bucket, Prefix=prefix, MaxKeys=1000, Marker=res['NextMarker'])\n",
    "        array = array + res.Contents\n",
    "    df = pd.DataFrame.from_records(array)\n",
    "    df.drop(columns=['ETag', 'StorageClass', 'Owner'], inplace=True)\n",
    "    df.to_parquet(name +'.parquet')\n",
    "    s3.upload_file(name +'.parquet', bucket, prefix + '/' + name +'.parquet', ExtraArgs={'ContentType': 'application/octet-stream'})\n",
    "        \n",
    "def get_lidar(project, token_uri, folder, bucket, target: str = 'data'):\n",
    "    s3 = boto3.client('s3',\n",
    "                endpoint_url=os.environ.get('S3_ENDPOINT_URL'),\n",
    "                aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "    copy_files(project, f\"mimeType='application/vnd.google-apps.folder' and name='{folder}'\", s3, bucket, f\"city-data/lidar/{target}\", token_uri)\n",
    "    doc_files(s3, bucket, f\"city-data/lidar/{target}\", \"lidar\")\n",
    "        \n",
    "def get_dtm(project, token_uri, folder, bucket, target: str = 'data'):\n",
    "    s3 = boto3.client('s3',\n",
    "                endpoint_url=os.environ.get('S3_ENDPOINT_URL'),\n",
    "                aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "    copy_files(project, f\"mimeType='application/vnd.google-apps.folder' and name='{folder}'\", s3, bucket, f\"city-data/dtm/{target}\", token_uri)\n",
    "    doc_files(s3, bucket, f\"city-data/dtm/{target}\", \"dtm\")\n",
    "    \n",
    "def get_dsm(project, token_uri, folder, bucket, target: str = 'data'):\n",
    "    s3 = boto3.client('s3',\n",
    "                endpoint_url=os.environ.get('S3_ENDPOINT_URL'),\n",
    "                aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    copy_files(project, f\"mimeType='application/vnd.google-apps.folder' and name='{folder}'\", s3, bucket, f\"city-data/dsm/{target}\", token_uri)\n",
    "    doc_files(s3, bucket, f\"city-data/dsm/{target}\", \"dsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d469ffdd-7b8c-4075-9b61-45b2b4ad318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_lidar = project.new_function(\n",
    "    name=\"download-lidar\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    source={\"source\": \"src/download-data.py\", \"handler\": \"get_lidar\"},\n",
    "    requirements=['google-api-python-client', 'google_auth_oauthlib']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e724f5e-85a0-4875-bfd7-0581029b60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f773041c-2c2f-4dfa-a8a1-e94a26d7df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lidar = func_download_lidar.run(action=\"job\", inputs={'token_uri': token_uri.key}, parameters={'folder': 'LAS_CLASSIFICATO_TEST', 'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3a97b8c-d6aa-4278-85ea-d71736776c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_dtm = project.new_function(\n",
    "    name=\"download-dtm\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    source={\"source\": \"src/download-data.py\", \"handler\": \"get_dtm\"},\n",
    "    requirements=['google-api-python-client', 'google_auth_oauthlib']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23a7e71a-1d05-4f1e-b218-27228328dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dtm = func_download_dtm.run(action=\"job\", inputs={'token_uri': token_uri.key}, parameters={'folder': 'DTM_0.5_TEST', 'bucket': 'datalake'}, local_execution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bee5f92-ac13-404e-9e24-ca6bdfac27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_download_dsm = project.new_function(\n",
    "    name=\"download-dsm\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    source={\"source\": \"src/download-data.py\", \"handler\": \"get_dsm\"},\n",
    "    requirements=['google-api-python-client', 'google_auth_oauthlib']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a4f8af4-4d77-41f9-8bc7-eabb67f804f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dsm = func_download_dsm.run(action=\"job\", inputs={'token_uri': token_uri.key}, parameters={'folder': 'DSM_0.5_TEST', 'bucket': 'datalake'}, local_execution=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
